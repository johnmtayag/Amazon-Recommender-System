{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07913076-e9a7-4505-998f-1bbc1ecfe072",
   "metadata": {},
   "source": [
    "## Locating the PC Boundaries for Anomalies\n",
    "\n",
    "* Create a simplified Isolation Forest algorithm that works within PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b68d003a-2316-44f9-8f7d-3f6fa039d2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-23qlritx because the default path (/home/jovyan/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "### Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "\n",
    "import ipyleaflet as ipy\n",
    "\n",
    "from pyspark import SparkContext\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.ml.functions as M\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql import Window as W\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import PCA, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59fcc4be-886c-4846-895f-ff7cfb405a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver memory: 64g\n",
      "Executor memory: 8g\n",
      "Number of executors: 7\n",
      "\n",
      "Initializing SparkContext\n",
      "<pyspark.sql.session.SparkSession object at 0x155551ca2910>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://exp-1-37.expanse.sdsc.edu:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x155551ca2910>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Notes from prof's example notebook\n",
    "# #Make sure you allocate enough memory per core. if you chose 3 cores you should select 6GB in your per Node setting. (presumably 2GB per core)\n",
    "\n",
    "### For server\n",
    "# 32 nodes\n",
    "# 64g\n",
    "\n",
    "## Start Spark context\n",
    "total_nodes = 8\n",
    "memory_per_node = 64\n",
    "\n",
    "driver_memory = f\"{memory_per_node}g\"\n",
    "executor_memory = f\"{int(memory_per_node/total_nodes)}g\"\n",
    "n_executors = total_nodes - 1\n",
    "print(f\"Driver memory: {driver_memory}\\nExecutor memory: {executor_memory}\\nNumber of executors: {n_executors}\\n\")\n",
    "try:\n",
    "    print(\"Initializing SparkContext\")\n",
    "    sc = SparkSession.builder.config(\"spark.driver.memory\", driver_memory) \\\n",
    "                             .config(\"spark.executor.memory\", executor_memory) \\\n",
    "                             .config('spark.local.dir', \"test_dir/\") \\\n",
    "                             .config(\"spark.driver.maxResultSize\", \"16g\") \\\n",
    "                             .config(\"spark.executor.instances\", n_executors) \\\n",
    "                             .getOrCreate()\n",
    "    \n",
    "                             # .config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml_2.12:0.9.4\") \\\n",
    "                             # .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\") \\\n",
    "except:\n",
    "    print(\"Starting new SparkContext\")\n",
    "    sc.stop()\n",
    "    sc = SparkSession.builder.config(\"spark.driver.memory\", driver_memory) \\\n",
    "                             .config(\"spark.executor.memory\", executor_memory) \\\n",
    "                             .config('spark.local.dir', \"test_dir/\") \\\n",
    "                             .config(\"spark.driver.maxResultSize\", \"16g\") \\\n",
    "                             .config(\"spark.executor.instances\", n_executors) \\\n",
    "                             .appName(\"MyApp\") \\\n",
    "                             .config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml_2.12:0.9.4\") \\\n",
    "                             .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\") \\\n",
    "                             .getOrCreate()\n",
    "print(sc)\n",
    "\n",
    "# Start SQL Context\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Add sc parameters\n",
    "sc.getActiveSession()\n",
    "# sc.builder.appName(\"Read CSV\").getOrCreate()\n",
    "# sc.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4de8490c-af75-47b4-87ab-89752571bbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|ss_id|stamp_date|     power_kW_values|     reconstructions|           recon_PC1|           recon_PC2|\n",
      "+---+-----+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0| 2405|2012-01-21|[0.0, 0.0, 0.0, 0...|[0.00410907621307...|-0.41782595826965074|-0.02458836598737569|\n",
      "+---+-----+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "CPU times: user 82.1 ms, sys: 5.73 ms, total: 87.8 ms\n",
      "Wall time: 3.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### Load datasets\n",
    "\n",
    "### Open the preprocessed dataset\n",
    "df = sqlContext.read.load(\"preprocessed_df_subset/preprocessed_df_subset.parquet\") \\\n",
    "               .select(\"id\", \"ss_id\", \"stamp_date\", \"power_kW_values\", \"reconstructions\", \"recon_PC1\", \"recon_PC2\")\n",
    "# df_count = df.count()\n",
    "\n",
    "# metadata\n",
    "meta_filename = \"metadata_preprocessed.csv\"\n",
    "df_meta = sc.read.csv(meta_filename, inferSchema=True, header=True)\n",
    "\n",
    "### Since metadata table is so small, convert to Pandas\n",
    "df_meta = df_meta.toPandas()\n",
    "\n",
    "df.show(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f5a801-3ab0-46bd-bedc-f9a9664bc5e0",
   "metadata": {},
   "source": [
    "## Initial filtering of close outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c4623-0d1a-4f27-bc9d-5fbfbbfed341",
   "metadata": {},
   "source": [
    "Set initial close-outlier cutoffs at PC1 > 2 and PC2 > 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8668d57-6352-4206-8f4a-5b0fa903e544",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 367 µs, sys: 922 µs, total: 1.29 ms\n",
      "Wall time: 11.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### Add a condition to filter out these outliers\n",
    "cond = (F.col(\"recon_PC1\")>1) | (F.col(\"recon_PC2\")>1)\n",
    "df2 = df.where(~cond)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45eecd3-33b0-475b-a956-b128dfb1650e",
   "metadata": {},
   "source": [
    "# Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e215652-b48f-48d3-ad27-a51c4b891d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get n splits on m columns randomly chosen\n",
    "\n",
    "def get_min_max_values(data, inputCols):\n",
    "    # Get the min and max value of columns in inputCols, output dictionary of dict[col] = (min, max)\n",
    "    return {i:data.select(F.min(col), F.max(col)).collect()[0] for i,col in enumerate(inputCols)}\n",
    "\n",
    "def get_line_params(data, inputCols, num_splits, offset_size, min_max_dict=False):\n",
    "    # Get a list of line parameters to randomly split values\n",
    "\n",
    "    def randomLine_parameters(data, min_max_dict, offset_size):\n",
    "        # Pick two random points within the boundaries of inputCols, then determine the parameters for the resulting line\n",
    "        # y-y1 = (y2-y1)/(x2-x1) * (x-x1)\n",
    "        # --> (y1-y2)*x + (x2-x1)*y + (x1-x2)*y1 - (y2-y1)*x1 = 0\n",
    "    \n",
    "        # Get offsets --> This ensures the selected points are closer to the mean (and away from boundaries)\n",
    "        # Having higher offsets is better when the data is more concentrated\n",
    "        col1_offset = (min_max_dict[0][1] - min_max_dict[0][0]) * offset_size\n",
    "        col2_offset = (min_max_dict[1][1] - min_max_dict[1][0]) * offset_size\n",
    "        \n",
    "        x1 = np.random.uniform(min_max_dict[0][0] + col1_offset, min_max_dict[0][1] - col1_offset)\n",
    "        x2 = np.random.uniform(min_max_dict[0][0] + col1_offset, min_max_dict[0][1] - col1_offset)\n",
    "        y1 = np.random.uniform(min_max_dict[1][0] + col2_offset, min_max_dict[1][1] - col2_offset)\n",
    "        y2 = np.random.uniform(min_max_dict[1][0] + col2_offset, min_max_dict[1][1] - col2_offset)\n",
    "    \n",
    "        # Get line parameters in standardized form\n",
    "        A = y2 - y1\n",
    "        B = x1 - x2\n",
    "        C = y1*(x2-x1) + x1*(y1-y2)\n",
    "        line_params = (A, B, C)\n",
    "        \n",
    "        return line_params\n",
    "    ############################################################################\n",
    "    # Variables\n",
    "    output = []\n",
    "    if min_max_dict is False:\n",
    "        min_max_dict = get_min_max_values(data, inputCols)\n",
    "    \n",
    "    ### Get a list of (A,B,C) tuples\n",
    "    for i in np.arange(0, num_splits):\n",
    "        output.append(randomLine_parameters(data, min_max_dict, offset_size))\n",
    "\n",
    "    return output\n",
    "\n",
    "def build_isolation_forest_simple(data, inputCols, outputCol, num_trees, num_splits, min_max_dict=False, offset_size = 0.001):\n",
    "    ### Build a custom isolation forest (simplified without sampling or recursive structure)\n",
    "    # Inputs:\n",
    "    #    Data --> Input dataframe\n",
    "    #    inputCols --> The columns to split on (recon_PC1 and recon_PC2 in this code)\n",
    "    #    num_splits --> The number of splits to run on each tree\n",
    "    #    random_offset --> An offset from the edges of the column split ranges to ensure the split stays within the range\n",
    "    #\n",
    "    # Determine a list of tuple-pairings of randomly selected columns and respective values to split on\n",
    "    # For each split:\n",
    "    #     Determine whether each point falls to the left (group 0) or right (group 1) of the split\n",
    "    #     Count the number of points in group 0 and group 1\n",
    "    #     Record the resulting fraction for each point and add the fraction to the running sum (tree_scoreSum)\n",
    "    # After all splits are complete, take the average splitScore as scoreSum\n",
    "    # After all trees are complete, take the average scoreSum and output as score\n",
    "    #\n",
    "    # Points that are outliers should have smaller scores than normal points\n",
    "\n",
    "    # Record the columns and get row count\n",
    "    print(\"Model Start\")\n",
    "    print()\n",
    "    model_start = time.time()\n",
    "    df_columns = data.schema.names\n",
    "\n",
    "    # Initialize the score column and necessary columns to handle the sampling, id column\n",
    "    output = data.withColumn(outputCol, F.lit(0))  \\\n",
    "                 .withColumn(\"scoreSum\", F.lit(0))\n",
    "\n",
    "    # Get the number of rows in the data\n",
    "    rowCount = output.count()\n",
    "\n",
    "    ### Start the trees\n",
    "    for j in np.arange(0, num_trees):\n",
    "        print(f\"Tree {j+1}/{num_trees} start:\")\n",
    "        tree_start = time.time()\n",
    "        # Find the columns and respective values to split on\n",
    "        line_param_list = get_line_params(output, inputCols, num_splits, offset_size, min_max_dict)\n",
    "        # Instantiate the tree_scoreSum column\n",
    "        output = output.withColumn(\"tree_scoreSum\", F.lit(0))\n",
    "        \n",
    "        ### Start the splits\n",
    "        split_start = time.time()\n",
    "        for i,(line_params) in enumerate(line_param_list):\n",
    "            if (((i-1)%10 == 0) & (i!=0)):\n",
    "                split_start = time.time()\n",
    "            A,B,C = line_params\n",
    "            # Mark each selected row as being on one side of the split\n",
    "            output = output.withColumn(\"which_side\", F.when(F.lit(A)*F.col(inputCols[0]) + F.lit(B)*F.col(inputCols[1]) + C >=0, 1).otherwise(0))\n",
    "            # Get the fraction of rows on either side of the split\n",
    "            group1 = output.select(F.sum(\"which_side\")).collect()[0][0] / rowCount\n",
    "            group0 = 1 - group1\n",
    "            # Add 1 to the score of the larger group\n",
    "            if group1 > group0:\n",
    "                group1 += 1\n",
    "            else:\n",
    "                group0 += 1\n",
    "    \n",
    "            # For each selected row, assign group0 or group1 to splitScore\n",
    "            output = output.withColumn(\"splitScore\", F.when(F.col(\"which_side\")==1, F.lit(group1)).otherwise(F.lit(group0))) \\\n",
    "    \n",
    "            # Add the splitScore to the running total tree_scoreSum\n",
    "            output = output.withColumn(\"tree_scoreSum\", F.col(\"tree_scoreSum\") + F.col(\"splitScore\"))\n",
    "\n",
    "            # if ((i+1)%10) == 0:\n",
    "                # print(f\"  Split {i+1}/{num_splits}: Time elapsed --> {time.time() - split_start}\")\n",
    "        ### Add the score to the overall running score total (scoreSum)\n",
    "        # 2*num_splits because of the extra weight added to the bigger group\n",
    "        output = output.withColumn(\"scoreSum\", F.col(\"scoreSum\") + (F.col(\"tree_scoreSum\") / F.lit(2*num_splits)))\n",
    "        \n",
    "        print(f\"Tree {j+1}:  Time elapsed --> {time.time() - tree_start}\")\n",
    "        print()\n",
    "\n",
    "    # Add the average score\n",
    "    output = output.withColumn(outputCol, F.round(F.col(\"scoreSum\") / F.lit(num_trees), 5))\n",
    "\n",
    "    ### Calculate the overall score and output it\n",
    "    output = output.select(df_columns + [outputCol]) \\\n",
    "                   .orderBy(outputCol, ascending=True)\n",
    "    \n",
    "    print(f\"Model complete: Time elapsed --> {time.time() - model_start}\")\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0406c76b-568d-4f36-9077-162241ad168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Optionally Scale the PC variables\n",
    "assembler1 = VectorAssembler(inputCols=[\"recon_PC1\"], outputCol=\"recon_PC1_vec\")\n",
    "assembler2 = VectorAssembler(inputCols=[\"recon_PC2\"], outputCol=\"recon_PC2_vec\")\n",
    "df3 = assembler1.transform(assembler2.transform(df2))\n",
    "\n",
    "mmScaler_pc1 = MinMaxScaler(inputCol=\"recon_PC1_vec\", outputCol=\"recon_PC1_scaled\")\n",
    "model1 = mmScaler_pc1.fit(df3)\n",
    "\n",
    "mmScaler_pc2 = MinMaxScaler(inputCol=\"recon_PC2_vec\", outputCol=\"recon_PC2_scaled\")\n",
    "model2 = mmScaler_pc2.fit(df3)\n",
    "\n",
    "df3 = model2.transform(model1.transform(df3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "317c528d-5b61-4c30-babb-69e04a6bc7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+--------------------+--------------------+------------------+------------------+\n",
      "| id|ss_id|stamp_date|           recon_PC1|           recon_PC2|  recon_PC1_scaled|  recon_PC2_scaled|\n",
      "+---+-----+----------+--------------------+--------------------+------------------+------------------+\n",
      "|  0| 2405|2012-01-21|-0.41782595826965074|-0.02458836598737569|0.8253311997502791|0.4839477701942617|\n",
      "+---+-----+----------+--------------------+--------------------+------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Select only the necessary variables\n",
    "df3 = df3.select(\"id\", \"ss_id\", \"stamp_date\", \"recon_PC1\", \"recon_PC2\",\n",
    "                   M.vector_to_array(F.col(\"recon_PC1_scaled\"))[0].alias(\"recon_PC1_scaled\"),\n",
    "                   M.vector_to_array(F.col(\"recon_PC2_scaled\"))[0].alias(\"recon_PC2_scaled\"))\n",
    "\n",
    "df3.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da1bdacd-702b-4250-abf8-44ebf81e97b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, Row(min(recon_PC1_scaled)=0.0, max(recon_PC1_scaled)=1.0))\n",
      "(1, Row(min(recon_PC2_scaled)=0.0, max(recon_PC2_scaled)=1.0))\n",
      "CPU times: user 7.02 ms, sys: 2.19 ms, total: 9.21 ms\n",
      "Wall time: 36.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### Run the model on a number of splits\n",
    "### Setting parameters\n",
    "#################################################################################\n",
    "inputCols = [\"recon_PC1_scaled\", \"recon_PC2_scaled\"]\n",
    "outputCol = \"score\"\n",
    "#################################################################################\n",
    "\n",
    "# Get input columns and corresponding min/max values\n",
    "min_max_dict = get_min_max_values(df3, inputCols)\n",
    "\n",
    "for item in min_max_dict.items():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7f900bf-f62f-41f3-bd3d-541414195ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1t_10s_std_score Column<'((POWER((score_0_1t_10s - 1t_10s_mean_score), 2) + POWER((score_1_1t_10s - 1t_10s_mean_score), 2)) + POWER((score_2_1t_10s - 1t_10s_mean_score), 2))'>\n",
      "1\n",
      "3t_10s_std_score Column<'((POWER((score_0_3t_10s - 3t_10s_mean_score), 2) + POWER((score_1_3t_10s - 3t_10s_mean_score), 2)) + POWER((score_2_3t_10s - 3t_10s_mean_score), 2))'>\n",
      "2\n",
      "5t_10s_std_score Column<'((POWER((score_0_5t_10s - 5t_10s_mean_score), 2) + POWER((score_1_5t_10s - 5t_10s_mean_score), 2)) + POWER((score_2_5t_10s - 5t_10s_mean_score), 2))'>\n",
      "3\n",
      "5t_30s_std_score Column<'((POWER((score_0_5t_30s - 5t_30s_mean_score), 2) + POWER((score_1_5t_30s - 5t_30s_mean_score), 2)) + POWER((score_2_5t_30s - 5t_30s_mean_score), 2))'>\n",
      "4\n",
      "3t_30s_std_score Column<'((POWER((score_0_3t_30s - 3t_30s_mean_score), 2) + POWER((score_1_3t_30s - 3t_30s_mean_score), 2)) + POWER((score_2_3t_30s - 3t_30s_mean_score), 2))'>\n",
      "5\n",
      "1t_30s_std_score Column<'((POWER((score_0_1t_30s - 1t_30s_mean_score), 2) + POWER((score_1_1t_30s - 1t_30s_mean_score), 2)) + POWER((score_2_1t_30s - 1t_30s_mean_score), 2))'>\n",
      "Complete\n",
      "\n",
      "CPU times: user 7.03 ms, sys: 3.58 ms, total: 10.6 ms\n",
      "Wall time: 39.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run a series of tests at differing numbers of trees and splits\n",
    "\n",
    "n_runs = 3\n",
    "run_vec = ((1,10), (3,10), (5,10), (5,30), (3,30), (1,30))\n",
    "\n",
    "for run_num in np.arange(0,len(run_vec)):\n",
    "    num_trees = run_vec[run_num][0]\n",
    "    num_splits = run_vec[run_num][1]\n",
    "    \n",
    "    print(run_num)\n",
    "    \n",
    "    # Initialize df_iforest\n",
    "    df_iforest = df3.cache()\n",
    "    df_iforest.count()\n",
    "    select_columns = df_iforest.schema.names\n",
    "    output_columns = []\n",
    "    # df_iforest = df2.select(select_columns).cache()\n",
    "    \n",
    "    for i in np.arange(0,3):\n",
    "        run_start = time.time()\n",
    "        print(f\"\\n\\nRun {i+1} --> Start\")\n",
    "        outputCol = f\"score_{i}_{num_trees}t_{num_splits}s\"\n",
    "        df_iforest = build_isolation_forest_simple(data=df_iforest, \n",
    "                                                   inputCols=inputCols, \n",
    "                                                   outputCol=outputCol,\n",
    "                                                   num_trees=num_trees,\n",
    "                                                   num_splits=num_splits,\n",
    "                                                   min_max_dict=min_max_dict)\n",
    "        select_columns = select_columns + [outputCol]\n",
    "        output_columns.append(outputCol)\n",
    "        print(f\"Done --> Total runtime elapsed: {time.time() - run_start}\")\n",
    "    \n",
    "    run_config = f\"{num_trees}t_{num_splits}s\"\n",
    "    score_cols = [f\"\\'score_{j}_{run_config}\\'\" for j in np.arange(0,n_runs)]\n",
    "    score_columns = [f\"F.col(\\'score_{j}_{run_config}\\')\" for j in np.arange(0,n_runs)]\n",
    "    mean_func = eval(\" + \".join(score_columns)) / len(score_columns)\n",
    "    std_string = f\"F.col(\\'{run_config}_mean_score\\'))**2\"\n",
    "    std_func =  eval(f\"((F.col({score_cols[0]})-{std_string} + (F.col({score_cols[1]})-{std_string} + (F.col({score_cols[2]})-{std_string})\")  ### Change this as needed\n",
    "\n",
    "    print(f\"{run_config}_std_score\", std_func)\n",
    "    \n",
    "    df_iforest = df_iforest.withColumn(f\"{run_config}_mean_score\", mean_func).withColumn(f\"{run_config}_std_score\", std_func)\n",
    "    \n",
    "    print(f\"\\nAll Runs complete --> Saving model to outliers_df_iforest/df_iforest_{num_trees}trees_{num_splits}splits\")\n",
    "    df_iforest.repartition(20).write.mode(\"overwrite\").parquet(f\"datasets/df_iforest_{num_trees}trees_{num_splits}splits\")\n",
    "\n",
    "print(\"Complete\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a1aa726-8ea1-4c82-a285-16a2f583660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iforest.unpersist().count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
