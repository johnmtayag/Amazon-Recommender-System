{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07913076-e9a7-4505-998f-1bbc1ecfe072",
   "metadata": {},
   "source": [
    "## Locating the PC Boundaries for Anomalies\n",
    "\n",
    "* Create a simplified Isolation Forest algorithm that works within PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b68d003a-2316-44f9-8f7d-3f6fa039d2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary cache directory at /tmp/matplotlib-6q6ir_m9 because the default path (/home/jovyan/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "### Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "\n",
    "import ipyleaflet as ipy\n",
    "\n",
    "from pyspark import SparkContext\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.ml.functions as M\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql import Window as W\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import PCA, VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59fcc4be-886c-4846-895f-ff7cfb405a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver memory: 100g\n",
      "Executor memory: 10g\n",
      "Number of executors: 9\n",
      "\n",
      "Initializing SparkContext\n",
      "<pyspark.sql.session.SparkSession object at 0x15551e220350>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://exp-1-37.expanse.sdsc.edu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x15551e220350>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### For server\n",
    "# 32 nodes\n",
    "# 64g\n",
    "\n",
    "## Start Spark context\n",
    "total_nodes = 10\n",
    "memory_per_node = 100\n",
    "\n",
    "driver_memory = f\"{memory_per_node}g\"\n",
    "executor_memory = f\"{int(memory_per_node/total_nodes)}g\"\n",
    "n_executors = total_nodes - 1\n",
    "print(f\"Driver memory: {driver_memory}\\nExecutor memory: {executor_memory}\\nNumber of executors: {n_executors}\\n\")\n",
    "try:\n",
    "    print(\"Initializing SparkContext\")\n",
    "    sc = SparkSession.builder.config(\"spark.driver.memory\", driver_memory) \\\n",
    "                             .config(\"spark.executor.memory\", executor_memory) \\\n",
    "                             .config('spark.local.dir', \"test_dir/\") \\\n",
    "                             .config(\"spark.driver.maxResultSize\", \"16g\") \\\n",
    "                             .config(\"spark.executor.instances\", n_executors) \\\n",
    "                             .getOrCreate()\n",
    "except:\n",
    "    print(\"Starting new SparkContext\")\n",
    "    sc.stop()\n",
    "    sc = SparkSession.builder.config(\"spark.driver.memory\", driver_memory) \\\n",
    "                             .config(\"spark.executor.memory\", executor_memory) \\\n",
    "                             .config('spark.local.dir', \"test_dir/\") \\\n",
    "                             .config(\"spark.driver.maxResultSize\", \"16g\") \\\n",
    "                             .config(\"spark.executor.instances\", n_executors) \\\n",
    "                             .appName(\"MyApp\") \\\n",
    "                             .config(\"spark.jars.packages\", \"com.microsoft.azure:synapseml_2.12:0.9.4\") \\\n",
    "                             .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\") \\\n",
    "                             .getOrCreate()\n",
    "print(sc)\n",
    "\n",
    "# Start SQL Context\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "sc.getActiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4de8490c-af75-47b4-87ab-89752571bbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|ss_id|stamp_date|     power_kW_values|     reconstructions|           recon_PC1|           recon_PC2|\n",
      "+---+-----+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0| 2405|2012-01-21|[0.0, 0.0, 0.0, 0...|[0.00410907621307...|-0.41782595826965074|-0.02458836598737569|\n",
      "+---+-----+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "CPU times: user 96 ms, sys: 8.13 ms, total: 104 ms\n",
      "Wall time: 4.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### Load datasets\n",
    "\n",
    "### Open the preprocessed dataset\n",
    "################################################################################### Ensure that the path here matches the path of the file from preprocessing\n",
    "df = sqlContext.read.load(\"preprocessed_df_subset/preprocessed_df_subset.parquet\") \\\n",
    "               .select(\"id\", \"ss_id\", \"stamp_date\", \"power_kW_values\", \"reconstructions\", \"recon_PC1\", \"recon_PC2\")\n",
    "# df_count = df.count()\n",
    "\n",
    "# metadata\n",
    "meta_filename = \"metadata_preprocessed.csv\"\n",
    "df_meta = sc.read.csv(meta_filename, inferSchema=True, header=True)\n",
    "\n",
    "### Since metadata table is so small, convert to Pandas\n",
    "df_meta = df_meta.toPandas()\n",
    "\n",
    "df.show(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f5a801-3ab0-46bd-bedc-f9a9664bc5e0",
   "metadata": {},
   "source": [
    "## Initial filtering of close outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c4623-0d1a-4f27-bc9d-5fbfbbfed341",
   "metadata": {},
   "source": [
    "Set initial close-outlier cutoffs at PC1 > 2 and PC2 > 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8668d57-6352-4206-8f4a-5b0fa903e544",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.16 ms, sys: 124 Âµs, total: 1.29 ms\n",
      "Wall time: 12.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### Add a condition to filter out these outliers\n",
    "cond = (F.col(\"recon_PC1\")>1) | (F.col(\"recon_PC2\")>1)\n",
    "df2 = df.where(~cond)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45eecd3-33b0-475b-a956-b128dfb1650e",
   "metadata": {},
   "source": [
    "# Isolation Forest\n",
    "Algorithm and dataset processing in this notebook only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e215652-b48f-48d3-ad27-a51c4b891d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get n splits on m columns randomly chosen\n",
    "\n",
    "def get_min_max_values(data, inputCols):\n",
    "    # Get the min and max value of columns in inputCols, output dictionary of dict[col] = (min, max)\n",
    "    return {i:data.select(F.min(col), F.max(col)).collect()[0] for i,col in enumerate(inputCols)}\n",
    "\n",
    "def get_line_params(data, inputCols, num_splits, offset_size, min_max_dict=False):\n",
    "    # Get a list of line parameters to randomly split values\n",
    "\n",
    "    def randomLine_parameters(data, min_max_dict, offset_size):\n",
    "        # Pick two random points within the boundaries of inputCols, then determine the parameters for the resulting line\n",
    "        # y-y1 = (y2-y1)/(x2-x1) * (x-x1)\n",
    "        # --> (y1-y2)*x + (x2-x1)*y + (x1-x2)*y1 - (y2-y1)*x1 = 0\n",
    "    \n",
    "        # Get offsets --> This ensures the selected points are closer to the mean (and away from boundaries)\n",
    "        # Having higher offsets is better when the data is more concentrated\n",
    "        col1_offset = (min_max_dict[0][1] - min_max_dict[0][0]) * offset_size\n",
    "        col2_offset = (min_max_dict[1][1] - min_max_dict[1][0]) * offset_size\n",
    "        \n",
    "        x1 = np.random.uniform(min_max_dict[0][0] + col1_offset, min_max_dict[0][1] - col1_offset)\n",
    "        x2 = np.random.uniform(min_max_dict[0][0] + col1_offset, min_max_dict[0][1] - col1_offset)\n",
    "        y1 = np.random.uniform(min_max_dict[1][0] + col2_offset, min_max_dict[1][1] - col2_offset)\n",
    "        y2 = np.random.uniform(min_max_dict[1][0] + col2_offset, min_max_dict[1][1] - col2_offset)\n",
    "    \n",
    "        # Get line parameters in standardized form\n",
    "        A = y2 - y1\n",
    "        B = x1 - x2\n",
    "        C = y1*(x2-x1) + x1*(y1-y2)\n",
    "        line_params = (A, B, C)\n",
    "        \n",
    "        return line_params\n",
    "    ############################################################################\n",
    "    # Variables\n",
    "    output = []\n",
    "    if min_max_dict is False:\n",
    "        min_max_dict = get_min_max_values(data, inputCols)\n",
    "    \n",
    "    ### Get a list of (A,B,C) tuples\n",
    "    for i in np.arange(0, num_splits):\n",
    "        output.append(randomLine_parameters(data, min_max_dict, offset_size))\n",
    "\n",
    "    return output\n",
    "\n",
    "def build_isolation_forest_simple(data, inputCols, outputCol, num_trees, num_splits, min_max_dict=False, offset_size = 0.001):\n",
    "    ### Build a custom isolation forest (simplified without sampling or recursive structure)\n",
    "    # Inputs:\n",
    "    #    Data --> Input dataframe\n",
    "    #    inputCols --> The columns to split on (recon_PC1 and recon_PC2 in this code)\n",
    "    #    num_splits --> The number of splits to run on each tree\n",
    "    #    random_offset --> An offset from the edges of the column split ranges to ensure the split stays within the range\n",
    "    #\n",
    "    # Determine a list of tuple-pairings of randomly selected columns and respective values to split on\n",
    "    # For each split:\n",
    "    #     Determine whether each point falls to the left (group 0) or right (group 1) of the split\n",
    "    #     Count the number of points in group 0 and group 1\n",
    "    #     Record the resulting fraction for each point and add the fraction to the running sum (tree_scoreSum)\n",
    "    # After all splits are complete, take the average splitScore as scoreSum\n",
    "    # After all trees are complete, take the average scoreSum and output as score\n",
    "    #\n",
    "    # Points that are outliers should have smaller scores than normal points\n",
    "\n",
    "    # Record the columns and get row count\n",
    "    print(\"Model Start\")\n",
    "    print()\n",
    "    model_start = time.time()\n",
    "    df_columns = data.schema.names\n",
    "\n",
    "    # Initialize the score column and necessary columns to handle the sampling, id column\n",
    "    output = data.withColumn(outputCol, F.lit(0))  \\\n",
    "                 .withColumn(\"scoreSum\", F.lit(0))\n",
    "\n",
    "    # Get the number of rows in the data\n",
    "    rowCount = output.count()\n",
    "\n",
    "    ### Start the trees\n",
    "    for j in np.arange(0, num_trees):\n",
    "        print(f\"Tree {j+1}/{num_trees} start:\")\n",
    "        tree_start = time.time()\n",
    "        # Find the columns and respective values to split on\n",
    "        line_param_list = get_line_params(output, inputCols, num_splits, offset_size, min_max_dict)\n",
    "        # Instantiate the tree_scoreSum column\n",
    "        output = output.withColumn(\"tree_scoreSum\", F.lit(0))\n",
    "        \n",
    "        ### Start the splits\n",
    "        split_start = time.time()\n",
    "        for i,(line_params) in enumerate(line_param_list):\n",
    "            if (((i-1)%10 == 0) & (i!=0)):\n",
    "                split_start = time.time()\n",
    "            A,B,C = line_params\n",
    "            # Mark each selected row as being on one side of the split\n",
    "            output = output.withColumn(\"which_side\", F.when(F.lit(A)*F.col(\"recon_PC1\") + F.lit(B)*F.col(\"recon_PC2\") + C >=0, 1).otherwise(0))\n",
    "            # Get the fraction of rows on either side of the split\n",
    "            group1 = output.select(F.sum(\"which_side\")).collect()[0][0] / rowCount\n",
    "            group0 = 1 - group1\n",
    "    \n",
    "            # For each selected row, assign group0 or group1 to splitScore\n",
    "            output = output.withColumn(\"splitScore\", F.when(F.col(\"which_side\")==1, F.lit(group1)).otherwise(F.lit(group0))) \\\n",
    "    \n",
    "            # Add the splitScore to the running total tree_scoreSum\n",
    "            output = output.withColumn(\"tree_scoreSum\", F.col(\"tree_scoreSum\") + F.col(\"splitScore\"))\n",
    "\n",
    "            if ((i+1)%10) == 0:\n",
    "                print(f\"  Split {i+1}/{num_splits}: Time elapsed --> {time.time() - split_start}\")\n",
    "        ### Add the score to the overall running score total (scoreSum)\n",
    "        output = output.withColumn(\"scoreSum\", F.col(\"scoreSum\") + (F.col(\"tree_scoreSum\") / F.lit(num_splits)))\n",
    "        \n",
    "        print(f\"Tree {j+1}:  Time elapsed --> {time.time() - tree_start}\")\n",
    "        print()\n",
    "\n",
    "    # Add the average score\n",
    "    output = output.withColumn(outputCol, F.round(F.col(\"scoreSum\") / F.lit(num_trees), 5))\n",
    "\n",
    "    ### Calculate the overall score and output it\n",
    "    output = output.select(df_columns + [outputCol]) \\\n",
    "                   .orderBy(outputCol, ascending=True)\n",
    "    \n",
    "    print(f\"Model complete: Time elapsed --> {time.time() - model_start}\")\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da1bdacd-702b-4250-abf8-44ebf81e97b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, Row(min(recon_PC1)=-7.031215404680069, max(recon_PC1)=0.9817973693571098))\n",
      "(1, Row(min(recon_PC2)=-0.7223526339584964, max(recon_PC2)=0.7194646672990974))\n",
      "CPU times: user 3.61 ms, sys: 1.94 ms, total: 5.55 ms\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### Run the model on a number of splits\n",
    "### Setting parameters\n",
    "#################################################################################\n",
    "inputCols = [\"recon_PC1\", \"recon_PC2\"]\n",
    "# num_trees = 5\n",
    "# num_splits = 100\n",
    "outputCol = \"score\"\n",
    "#################################################################################\n",
    "\n",
    "# Initialize df_iforest\n",
    "df_iforest = df2\n",
    "\n",
    "# Get input columns and corresponding min/max values\n",
    "min_max_dict = get_min_max_values(df_iforest, inputCols)\n",
    "\n",
    "for item in min_max_dict.items():\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7f900bf-f62f-41f3-bd3d-541414195ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "\n",
      "\n",
      "Run 1 --> Start\n",
      "Model Start\n",
      "\n",
      "Tree 1/1 start:\n",
      "  Split 10/30: Time elapsed --> 48.442001819610596\n",
      "  Split 20/30: Time elapsed --> 24.06851315498352\n",
      "  Split 30/30: Time elapsed --> 32.95797419548035\n",
      "Tree 1:  Time elapsed --> 114.57231664657593\n",
      "\n",
      "Model complete: Time elapsed --> 117.5252640247345\n",
      "\n",
      "Caching the output...\n",
      "Done --> Total runtime elapsed: 130.64413213729858\n",
      "\n",
      "\n",
      "Run 2 --> Start\n",
      "Model Start\n",
      "\n",
      "Tree 1/1 start:\n",
      "  Split 10/30: Time elapsed --> 3.6160566806793213\n",
      "  Split 20/30: Time elapsed --> 3.516334295272827\n",
      "  Split 30/30: Time elapsed --> 4.014955282211304\n",
      "Tree 1:  Time elapsed --> 12.262625694274902\n",
      "\n",
      "Model complete: Time elapsed --> 12.576621532440186\n",
      "\n",
      "Caching the output...\n",
      "Done --> Total runtime elapsed: 27.6690571308136\n",
      "\n",
      "\n",
      "Run 3 --> Start\n",
      "Model Start\n",
      "\n",
      "Tree 1/1 start:\n",
      "  Split 10/30: Time elapsed --> 4.7903361320495605\n",
      "  Split 20/30: Time elapsed --> 4.091628313064575\n",
      "  Split 30/30: Time elapsed --> 4.08524489402771\n",
      "Tree 1:  Time elapsed --> 14.265088319778442\n",
      "\n",
      "Model complete: Time elapsed --> 16.51615071296692\n",
      "\n",
      "Caching the output...\n",
      "Done --> Total runtime elapsed: 28.320733785629272\n",
      "\n",
      "\n",
      "Run 4 --> Start\n",
      "Model Start\n",
      "\n",
      "Tree 1/1 start:\n",
      "  Split 10/30: Time elapsed --> 3.6355412006378174\n",
      "  Split 20/30: Time elapsed --> 3.908647298812866\n",
      "  Split 30/30: Time elapsed --> 3.8218812942504883\n",
      "Tree 1:  Time elapsed --> 12.71387243270874\n",
      "\n",
      "Model complete: Time elapsed --> 12.961755990982056\n",
      "\n",
      "Caching the output...\n",
      "Done --> Total runtime elapsed: 32.72841143608093\n",
      "\n",
      "\n",
      "Run 5 --> Start\n",
      "Model Start\n",
      "\n",
      "Tree 1/1 start:\n",
      "  Split 10/30: Time elapsed --> 4.703004360198975\n",
      "  Split 20/30: Time elapsed --> 4.856440782546997\n",
      "  Split 30/30: Time elapsed --> 4.794835805892944\n",
      "Tree 1:  Time elapsed --> 15.995446920394897\n",
      "\n",
      "Model complete: Time elapsed --> 16.37062096595764\n",
      "\n",
      "Caching the output...\n",
      "Done --> Total runtime elapsed: 46.736669301986694\n",
      "\n",
      "All Runs complete --> Saving model to outliers_df_iforest/df_iforest_1trees_30splits\n",
      "Complete\n",
      "\n",
      "CPU times: user 369 ms, sys: 115 ms, total: 484 ms\n",
      "Wall time: 4min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Run a series of tests at differing numbers of trees and splits\n",
    "\n",
    "# 1t_10s #\n",
    "# 3t_10s #\n",
    "# 5t_10s #\n",
    "# 7t_10s #\n",
    "\n",
    "# 1t_30s\n",
    "# 3t_30s #\n",
    "# 5t_30s #\n",
    "# 7t_30s #\n",
    "\n",
    "# 1t_20s\n",
    "# 3t_20s\n",
    "# 5t_20s\n",
    "# 7t_20s\n",
    "\n",
    "run_num = 7\n",
    "run_vec = ((1,10), (3,10), (5,10), (7,10), (7,30), (5,30), (3,30), (1,30), (10, 50))\n",
    "num_trees = run_vec[run_num][0]\n",
    "num_splits = run_vec[run_num][1]\n",
    "\n",
    "print(run_num)\n",
    "\n",
    "# Initialize df_iforest\n",
    "select_columns = [\"id\", \"recon_PC1\", \"recon_PC2\"]\n",
    "output_columns = []\n",
    "df_iforest = df2.select(select_columns).cache()\n",
    "\n",
    "for i in np.arange(0,5):\n",
    "    run_start = time.time()\n",
    "    print(f\"\\n\\nRun {i+1} --> Start\")\n",
    "    outputCol = f\"score_{i}_{num_trees}t_{num_splits}s\"\n",
    "    df_iforest = build_isolation_forest_simple(data=df_iforest, \n",
    "                                               inputCols=inputCols, \n",
    "                                               outputCol=outputCol,\n",
    "                                               num_trees=num_trees,\n",
    "                                               num_splits=num_splits,\n",
    "                                               min_max_dict=min_max_dict)\n",
    "    select_columns = select_columns + [outputCol]\n",
    "    output_columns.append(outputCol)\n",
    "    print(\"\\nCaching the output...\")\n",
    "    df_iforest = df_iforest.cache()\n",
    "    df_iforest.count()\n",
    "    print(f\"Done --> Total runtime elapsed: {time.time() - run_start}\")\n",
    "\n",
    "print(f\"\\nAll Runs complete --> Saving model to outliers_df_iforest/df_iforest_{num_trees}trees_{num_splits}splits\")\n",
    "df_iforest.select([\"id\"] + output_columns).repartition(1).write.mode(\"overwrite\").parquet(f\"outliers_df_iforest/df_iforest_{num_trees}trees_{num_splits}splits\")\n",
    "\n",
    "print(\"Complete\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a1aa726-8ea1-4c82-a285-16a2f583660d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28087969"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_iforest.unpersist().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4820c72b-a3cc-464c-b553-90cc62dab808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+----------+--------------------+--------------------+-------------------+------------------+--------------+--------------+--------------+--------------+--------------+\n",
      "|          id|ss_id|stamp_date|     power_kW_values|     reconstructions|          recon_PC1|         recon_PC2|score_0_1t_10s|score_1_1t_10s|score_2_1t_10s|score_3_1t_10s|score_4_1t_10s|\n",
      "+------------+-----+----------+--------------------+--------------------+-------------------+------------------+--------------+--------------+--------------+--------------+--------------+\n",
      "|652835274355|13015|2016-05-24|[0.0, 0.0, 0.0, 0...|[6.09746580686240...|-2.2372372455956455|0.7194646672990974|       0.49986|       0.62795|       0.61327|       0.30982|       0.39662|\n",
      "+------------+-----+----------+--------------------+--------------------+-------------------+------------------+--------------+--------------+--------------+--------------+--------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "59dee832-c181-4c43-abaa-076c1cd66be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|ss_id|stamp_date|     power_kW_values|     reconstructions|           recon_PC1|           recon_PC2|\n",
      "+---+-----+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0| 2405|2012-01-21|[0.0, 0.0, 0.0, 0...|[0.00410907621307...|-0.41782595826965074|-0.02458836598737569|\n",
      "+---+-----+----------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b6050ea4-90a1-46cc-b2ff-706069d828cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate aggregate stats and save as a condensed file\n",
    "\n",
    "filenames = [\"outliers_df_iforest/df_iforest_1t10s.parquet\", \"outliers_df_iforest/df_iforest_1t30s.parquet\",\n",
    "             \"outliers_df_iforest/df_iforest_3t10s.parquet\", \"outliers_df_iforest/df_iforest_3t30s.parquet\",\n",
    "             \"outliers_df_iforest/df_iforest_5t10s.parquet\", \"outliers_df_iforest/df_iforest_5t30s.parquet\",\n",
    "             \"outliers_df_iforest/df_iforest_7t10s.parquet\", \"outliers_df_iforest/df_iforest_7t30s.parquet\"]\n",
    "\n",
    "score_SScolumns = ((F.col(\"score_5t_20s\")-F.col(\"mean_score\"))**2 +\n",
    "                   (F.col(\"score_5t_50s\")-F.col(\"mean_score\"))**2 +\n",
    "                   (F.col(\"score_3t_20s\")-F.col(\"mean_score\"))**2 +\n",
    "                   (F.col(\"score_3t_50s\")-F.col(\"mean_score\"))**2 +\n",
    "                   (F.col(\"score_3t_100s\")-F.col(\"mean_score\"))**2)\n",
    "\n",
    "df2_withScores = df2\n",
    "\n",
    "for i,(file, save) in enumerate(zip(filenames, savelocs)):\n",
    "    run_config = file.split(\".\")[0][-5:-3] + \"_\" + file.split(\".\")[0][-3:]\n",
    "    score_cols = [f\"\\'score_{j}_{run_config}\\'\" for j in np.arange(0,5)]\n",
    "    score_columns = [f\"F.col(\\'score_{j}_{run_config}\\')\" for j in np.arange(0,5)]\n",
    "    mean_func = eval(\" + \".join(score_columns)) / len(score_columns)\n",
    "    std_string = f\"F.col(\\'{run_config}_mean_score\\'))**2\"\n",
    "    std_func =  eval(f\"((F.col({score_cols[0]})-{std_string} + (F.col({score_cols[1]})-{std_string} + (F.col({score_cols[2]})-{std_string} + (F.col({score_cols[3]})-{std_string} + (F.col({score_cols[4]})-{std_string})\")\n",
    "\n",
    "    df3 = sqlContext.read.load(file).withColumn(f\"{run_config}_mean_score\", mean_func).withColumn(f\"{run_config}_std_score\", std_func)\n",
    "\n",
    "    df2_withScores = df2_withScores.join(df3.select(\"id\", f\"{run_config}_mean_score\", f\"{run_config}_std_score\"), on=\"id\", how=\"inner\")\n",
    "\n",
    "df2_withScores.repartition(1).write.mode(\"overwrite\").parquet(\"outliers_df_iforest/df_iforest_all_scores\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
